{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "import scipy.io\n",
    "from neural_networks import *\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('data/ex4data1.mat')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "Lambda = 1\n",
    "num_labels = np.unique(y).astype(int).size\n",
    "theta1 = rand_initialize_weights(input_layer_size, hidden_layer_size)\n",
    "theta2 = rand_initialize_weights(hidden_layer_size, num_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "input_layer_size = 3\n",
    "hidden_layer_size = 5\n",
    "num_labels = 3\n",
    "m = 5\n",
    "y  = 1 + np.mod(range(m), num_labels).T\n",
    "X  = debug_initialize_weights(m, input_layer_size - 1)\n",
    "Lambda = 0\n",
    "theta1 = debug_initialize_weights(hidden_layer_size, input_layer_size)\n",
    "theta2 = debug_initialize_weights(num_labels, hidden_layer_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "nn_params = unroll_parameters(theta1, theta2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "X = np.hstack((np.ones((m, 1)), X))\n",
    "labels = np.unique(y).astype(int)\n",
    "theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], \\\n",
    "                    (hidden_layer_size, input_layer_size + 1), order='F')\n",
    "theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \\\n",
    "                    (labels.size, hidden_layer_size + 1), order='F')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "z2 = X @ theta1.T\n",
    "a2 = sigmoid(z2)\n",
    "a2 = np.hstack((np.ones((a2.shape[0], 1)), a2))\n",
    "z3 = a2 @ theta2.T\n",
    "a3 = sigmoid(z3)\n",
    "y_matrix = np.empty((len(y), 0))\n",
    "for i in labels:\n",
    "    newy = (y == i).astype(int)\n",
    "    y_matrix = np.hstack((y_matrix, newy.reshape(-1, 1)))\n",
    "J = (1.0 / m) * np.sum((-y_matrix * np.log(a3)) - (1 - y_matrix) * np.log(1 - a3))\n",
    "sum_square_theta1 = np.sum(theta1[:, 1:] ** 2)\n",
    "sum_square_theta2 = np.sum(theta2[:, 1:] ** 2)\n",
    "J = J + ((Lambda / (2.0 * m)) * (sum_square_theta1 + sum_square_theta2))\n",
    "\n",
    "d3 = a3 - y_matrix\n",
    "theta2_grad = (a2.T @ d3).T + Lambda / m * theta2\n",
    "z2 = np.hstack((np.ones((z2.shape[0], 1)), z2))\n",
    "d2 = (d3 @ theta2) * sigmoid_gradient(z2)\n",
    "theta1_grad = 1 / m * (X.T @ d2).T\n",
    "theta1_grad = theta1_grad[1:theta1_grad.shape[0], :] + Lambda / m * theta1\n",
    "grad = unroll_parameters(theta1_grad, theta2_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Short hand for cost function\n",
    "def costFunc(p):\n",
    "    return nn_cost_function_reg(p, input_layer_size, hidden_layer_size, X, y, Lambda)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 5000 and the array at index 1 has size 5",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-3a1fb2b91223>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnumgrad\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_numerical_gradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcostFunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnn_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Google Drive/projects/Python/Courses Pluralsight/Practice Pycharm/MLStanfod/neural_networks.py\u001B[0m in \u001B[0;36mcompute_numerical_gradient\u001B[0;34m(J, theta)\u001B[0m\n\u001B[1;32m    230\u001B[0m         \u001B[0;31m# Set perturbation vector\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m         \u001B[0mperturb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mperturb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"F\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m         \u001B[0mloss1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mJ\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtheta\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mperturb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    233\u001B[0m         \u001B[0mloss2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mJ\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtheta\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mperturb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    234\u001B[0m         \u001B[0;31m# Compute Numerical Gradient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-31904a9d17ba>\u001B[0m in \u001B[0;36mcostFunc\u001B[0;34m(p)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Short hand for cost function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcostFunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mnn_cost_function_reg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_layer_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_layer_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mLambda\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Google Drive/projects/Python/Courses Pluralsight/Practice Pycharm/MLStanfod/neural_networks.py\u001B[0m in \u001B[0;36mnn_cost_function_reg\u001B[0;34m(nn_params, input_layer_size, hidden_layer_size, X, y, Lambda)\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0mnewy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0my_matrix\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_matrix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnewy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m     \u001B[0;31m# at this point, both a3 and y are m x k matrices, where m is the number of inputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[0;31m# and k is the number of hypotheses. Given that the cost function is a sum\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mhstack\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/MLStanfod/lib/python3.8/site-packages/numpy/core/shape_base.py\u001B[0m in \u001B[0;36mhstack\u001B[0;34m(tup)\u001B[0m\n\u001B[1;32m    343\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_nx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    344\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 345\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_nx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    346\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 5000 and the array at index 1 has size 5"
     ]
    }
   ],
   "source": [
    "numgrad = compute_numerical_gradient(costFunc, nn_params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}